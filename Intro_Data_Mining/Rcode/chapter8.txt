## bagging
#install.packages(c('tree','mlbench','adabag'))
library(tree)
library(mlbench)
data(Sonar)
clr = Sonar$Class; sonar = Sonar[,1:60]
snx = as.matrix(sonar)
sny = rep (1, 208); sny[which(clr == "R")] = 0
set.seed(120)
lst = sample(208)
tr = lst[1:145]
val = lst[146:208]
da = data.frame(y=clr, xx=snx)

fgl.tr = tree(y ~ ., data=da, subset=tr)
fgl.cv = cv.tree(fgl.tr, , prune.tree, K=10)
opt = fgl.cv$k[which.min(fgl.cv$dev)]
tt = prune.tree(fgl.tr, k=opt)
PP = predict(tt, da[val,-1], type="class")
mean(PP != clr[val])

library(adabag)
fit.bag = bagging(y ~., data=da[-val,], mfinal=50)
predict.bagging(fit.bag, newdata=da[val,])$error


## boosting
wine = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", 
           sep=",", header=T)
colnames(wine) = c("Type","Alcohol","Malic","Ash","Alcalinity","Magnesium",
                      "Phenols","Flavanoids","Nonflavanoids",
                      "Proanthocyanins","Color","Hue","Dilution","Proline")
lst = sample(nrow(wine)); tr = lst[1:100]; ts = lst[101:nrow(wine)]
library(gbm)
ds = wine[tr,]; ds$Type = as.numeric(ds$Type)
ds$Type[ds$Type>1] = 0;
ds1.gbm = gbm(Type ~ Alcohol + Malic + Ash + Alcalinity + Magnesium +
                Phenols + Flavanoids + Nonflavanoids +
                Proanthocyanins +
                Color + Hue + Dilution + Proline,
                data=ds, distribution="adaboost", n.trees=9000, cv.folds=5)
best1.iter = gbm.perf(ds1.gbm,method="cv")
print(best1.iter)

ds2.gbm = gbm(Type ~ Alcohol + Malic + Ash + Alcalinity + Magnesium +
                Phenols + Flavanoids + Nonflavanoids +
                Proanthocyanins +
                Color + Hue + Dilution + Proline,
                data=ds, distribution="bernoulli", n.trees=9000, cv.folds=5)
best2.iter = gbm.perf(ds2.gbm,method="cv")
print(best2.iter)

pp = predict(ds1.gbm,wine[ts,-1],type="response",n.trees=best1.iter)
pyy = ifelse(wine$Type[ts]>1, -1, 1)
mean(sign(pp) != pyy)

pp = predict(ds2.gbm,wine[ts,-1],type="response",n.trees=best2.iter)
pyy = ifelse(wine$Type[ts]>1, -1, 1)
mean(sign(pp-0.5) != pyy)

##  Random forest
#install.packages(c('randomForest','gbm'))
rm(list = ls())
setwd('E:\\데이터마이닝')
library(randomForest)
library(MASS)
library(gbm)

XY_tr = read.csv("LC_sample_tr.csv")
XY_ts = read.csv("LC_sample_ts.csv")
XY_tr = XY_tr[,-1]; XY_ts = XY_ts[,-1]
XY_tr[,4] = as.factor(XY_tr[,4]); XY_ts[,4] = as.factor(XY_ts[,4])

RF_res = randomForest(y ~ ., data=XY_tr, ntree=1000, Importance=TRUE)
summary(RF_res)
RF_res$importance
RF_res$confusion

PP = predict(RF_res, XY_ts[,1:3])
mean(PP != XY_ts[,4])


#################################################################################
setwd('E:\\데이터마이닝')
spamD <- read.table('https://raw.github.com/WinVector/zmPDSwR/master/Spambase/spamD.tsv',header=T,sep='\t')      
spamTrain <- subset(spamD,spamD$rgroup>=10)
spamTest <- subset(spamD,spamD$rgroup<10)

spamTrain <- subset(spamD,spamD$rgroup>=10)
spamTest <- subset(spamD,spamD$rgroup<10)

spamVars <- setdiff(colnames(spamD),list('rgroup','spam'))
spamFormula <- as.formula(paste('spam=="spam"',     
                          paste(spamVars,collapse=' + '),sep=' ~ '))

loglikelihood <- function(y, py) {      
  pysmooth <- ifelse(py==0, 1e-12,
                  ifelse(py==1, 1-1e-12, py))
  sum(y * log(pysmooth) + (1-y)*log(1 - pysmooth))
}

accuracyMeasures <- function(pred, truth, name="model") {   
  dev.norm <- -2*loglikelihood(as.numeric(truth), pred)/length(pred)    
  ctable <- table(truth=truth,
                 pred=(pred>0.5))                                       
  accuracy <- sum(diag(ctable))/sum(ctable)
  precision <- ctable[2,2]/sum(ctable[,2])
  recall <- ctable[2,2]/sum(ctable[2,])
  f1 <- 2*precision*recall/(precision+recall)
  data.frame(model=name, accuracy=accuracy, f1=f1, dev.norm)
}

library(rpart)    
treemodel <- rpart(spamFormula , spamTrain)
accuracyMeasures(predict(treemodel, newdata=spamTrain), 
                 spamTrain$spam=="spam",
                 name="tree, training")
accuracyMeasures(predict(treemodel, newdata=spamTest), 
                 spamTest$spam=="spam",
                 name="tree, test")

#############################################BAGGING
ntrain <- dim(spamTrain)[1]
n <- ntrain                  
ntree <- 100

samples <- sapply(1:ntree,          
                 FUN = function(iter)
                   {sample(1:ntrain, size=n, replace=T)})

treelist <-lapply(1:ntree,          
                  FUN=function(iter)
                  {samp <- samples[,iter];
                   rpart(spamFormula, spamTrain[samp,])})

predict.bag <- function(treelist, newdata) {    
  preds <- sapply(1:length(treelist),
                 FUN=function(iter) {
                   predict(treelist[[iter]], newdata=newdata)})
  predsums <- rowSums(preds)
  predsums/length(treelist)
}

accuracyMeasures(predict.bag(treelist, newdata=spamTrain),      
                 spamTrain$spam=="spam",
                 name="bagging, training")

accuracyMeasures(predict.bag(treelist, newdata=spamTest),
                 spamTest$spam=="spam",
                 name="bagging, test")
##############################################RANDOM FOREST
library(randomForest)               
set.seed(12345)   

fmodel <- randomForest(x=spamTrain[,spamVars],  
        y=spamTrain$spam,
        ntree=100,  
        nodesize=7,     
        importance=T)  

accuracyMeasures(predict(fmodel, 
   newdata=spamTrain[,spamVars], type='prob')[,'spam'],
   spamTrain$spam=="spam",name="random forest, train")

accuracyMeasures(predict(fmodel,
   newdata=spamTest[,spamVars],type='prob')[,'spam'],
   spamTest$spam=="spam",name="random forest, test")
varImp <- importance(fmodel)                
varImp[1:10, ]

