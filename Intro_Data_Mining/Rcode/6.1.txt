####feed-forward neural networks
library(nnet)
nn.iris <- nnet(Species~., data=iris, size=c(2,2), rang=.1, decay=5e-4, maxit=200)
summary(nn.iris)

library(devtools)
#source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
plot.nnet(nn.iris)

#install.packages('clusterGeneration')
library(clusterGeneration)
library(scales)
library(reshape)
plot(nn.iris)

table(iris$Species, predict(nn.iris, iris, type='class'))

# function evaluating test error rates for different numbers of hidden units
test.err = function(h.size){
    ir = nnet(Species~., data=iris, size = h.size,
        decay = 5e-4, trace=F)
    y = iris$Species
    p = predict(ir, iris, type = "class")
    err = mean(y != p)
    c(h.size, err)
}
# compare test error rates for neural networks with 2-10 hidden units
out = t(sapply(2:10, FUN = test.err))
plot(out, type="b", xlab="The number of Hidden units", ylab="Test Error")



###backpropagation neural networks
library(neuralnet)
net.iris <- neuralnet(Species~.,hidden=c(2,2), data=iris, linear.output=F, stepmax=1e+10)
net.iris
plot(net.iris)


#install.packages('neuralnet')
library(neuralnet)
data(infert)
net.infert <- neuralnet(case~parity+induced+spontaneous, 
						hidden=c(20,20), data=infert, linear.output=F)
net.infert

plot(net.infert)

head(net.infert$generalized.weights[[1]])
par(mfrow=c(2,2))
gwplot(net.infert, selected.covariate='age', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='parity', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='induced', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='spontaneous', min=-2.5, max=5)

############################################################
library(MASS)
Boston
apply(Boston,2,function(x) sum(is.na(x)))

index <- sample(1:nrow(Boston),round(0.75*nrow(Boston)))
train <- Boston[index,]
test <- Boston[-index,]
lm.fit <- glm(medv~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)
#################################
maxs <- apply(Boston, 2, max) 
mins <- apply(Boston, 2, min)
scaled <- as.data.frame(scale(Boston, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]

n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
net.Boston <- neuralnet(f,data=train_,hidden=c(5,3), linear.output=T)

plot(net.Boston)
net.Boston$result.matrix

pr.nn <- compute(net.Boston,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
test.r <- (test_$medv)*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
c(MSE.lm,MSE.nn)

par(mfrow=c(1,2))
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)


plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=1)
points(test$medv,pr.lm,col='blue',pch=18,cex=1)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))